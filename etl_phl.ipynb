{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wbendinelli/pneumonia_classification/blob/main/etl_phl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the main factors that determine post-harvest losses of grains?\n",
        "\n",
        "### Hypotheses to be tested\n",
        "\n",
        "H1. Macroeconomic conditions influence post-harvest losses (PHL) of grains.\n",
        "\n",
        "H2. Increase in food production and increase in PHL per capita are directly correlated.\n",
        "\n",
        "H3. Higher level of economic development of a country can bring a lower level of PHL.\n",
        "\n",
        "H4. Lack of food storage and food marketing infrastructure can lead to higher PHL.\n",
        "\n",
        "### Article abstract\n",
        "Reducing post-harvest losses (PHL) improves food security, safety, and profits for actors in the food supply chain. Despite its importance, most published studies on PHL have been qualitative due to data limitations. This paper aims to address this gap by investigating the impact of macroeconomic factors on PHL in developing countries using a quantitative approach. The study will contribute to the existing body of knowledge by providing empirical evidence on the relationship between macroeconomic factors and PHL, which can inform policy and practice aimed at reducing PHL and improving food security and safety."
      ],
      "metadata": {
        "id": "FnZoMbBIEdDg"
      },
      "id": "FnZoMbBIEdDg"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2zLXgWiYcbS",
        "outputId": "1aebf50c-0f82-42bf-d4ef-911bb6a5aab5"
      },
      "id": "B2zLXgWiYcbS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "128e4730-f4a2-484b-a7c3-3874517b9ecc",
      "metadata": {
        "id": "128e4730-f4a2-484b-a7c3-3874517b9ecc"
      },
      "outputs": [],
      "source": [
        "#libraries used in the study\n",
        "import pandas as pd\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8588a2a-037b-47e8-bf9e-d77cfa9fb504",
      "metadata": {
        "id": "f8588a2a-037b-47e8-bf9e-d77cfa9fb504"
      },
      "source": [
        "## Extracting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the list of files to keep\n",
        "keep_files = [\"FoodBalanceSheetsHistoric_E_All_Data_(Normalized).csv\",\n",
        "              \"Food_Security_Data_E_All_Data_(Normalized).csv\",\n",
        "              \"Trade_Indices_E_All_Data_(Normalized).csv\",\n",
        "              \"Prices_E_All_Data_(Normalized).csv\"]\n",
        "\n",
        "# Get the path to the folder containing the files\n",
        "folder_path = '/content/drive/MyDrive/ia_ml_projects/post_harvest_loss/fao_database/'\n",
        "\n",
        "# Change the current working directory to the folder\n",
        "current_dir = os.chdir(folder_path)\n",
        "\n",
        "# Iterate through all files in the current directory\n",
        "for file in os.listdir(current_dir):\n",
        "    # Check if the file is a ZIP file\n",
        "    if file.endswith(\".zip\"):\n",
        "        # Open the ZIP file\n",
        "        with ZipFile(file, 'r') as zip_ref:\n",
        "            # Extract all files from the ZIP file\n",
        "            zip_ref.extractall(current_dir)\n",
        "\n",
        "# Remove non-zip files and files not in the keep list\n",
        "for filename in os.listdir(current_dir):\n",
        "    if not filename.endswith(\".zip\") and filename not in keep_files:\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        os.remove(file_path)\n",
        "\n",
        "# Print a success message\n",
        "print(\"All non-zip files and files not in the keep list have been removed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfzKOKrlbZK1",
        "outputId": "8b0f63d3-bd3f-425e-88e3-696146296d6a"
      },
      "id": "mfzKOKrlbZK1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All non-zip files and files not in the keep list have been removed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a9fd39-ad64-4c26-9493-61045aa51d38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9a9fd39-ad64-4c26-9493-61045aa51d38",
        "outputId": "40300e6a-3612-4036-8350-da36fce3dcfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-30d5afb94712>:14: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  fsi_raw_data = pd.read_csv(path2, encoding = \"ISO-8859-1\")\n"
          ]
        }
      ],
      "source": [
        "#importing lists for the study\n",
        "path1 = '/content/drive/MyDrive/ia_ml_projects/post_harvest_loss/auxiliary_files/countries_list.xlsx'\n",
        "path2 = '/content/drive/MyDrive/ia_ml_projects/post_harvest_loss/auxiliary_files/fao_item_list.xlsx'\n",
        "\n",
        "countries_list = pd.read_excel(path1, sheet_name = 'list_economies', header=[2])\n",
        "fao_item_list = pd.read_excel(path2, sheet_name = 'item_description', header=[2])\n",
        "\n",
        "#on the FAOSTAT, download all data files in .zip format\n",
        "\n",
        "path1 = '/content/drive/MyDrive/ia_ml_projects/post_harvest_loss/fao_database/FoodBalanceSheetsHistoric_E_All_Data_(Normalized).csv'\n",
        "fbs_raw_data = pd.read_csv(path1, encoding = \"ISO-8859-1\")\n",
        "\n",
        "path2 = '/content/drive/MyDrive/ia_ml_projects/post_harvest_loss/fao_database/Food_Security_Data_E_All_Data_(Normalized).csv'\n",
        "fsi_raw_data = pd.read_csv(path2, encoding = \"ISO-8859-1\")\n",
        "\n",
        "path3 = '/content/drive/MyDrive/ia_ml_projects/post_harvest_loss/fao_database/Trade_Indices_E_All_Data_(Normalized).csv'\n",
        "trade_raw_data = pd.read_csv(path3, encoding = \"ISO-8859-1\")\n",
        "\n",
        "path4 = '/content/drive/MyDrive/ia_ml_projects/post_harvest_loss/fao_database/Prices_E_All_Data_(Normalized).csv'\n",
        "prices_raw_data = pd.read_csv(path4, encoding = \"ISO-8859-1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5723d8c0-189b-4ef2-b791-b3247153766c",
      "metadata": {
        "id": "5723d8c0-189b-4ef2-b791-b3247153766c"
      },
      "source": [
        "## Transforming"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7582c51-c552-4450-b2d1-2980898e1c9b",
      "metadata": {
        "id": "a7582c51-c552-4450-b2d1-2980898e1c9b"
      },
      "source": [
        "### Food balance sheets data transforming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4174f629-9688-4227-bc67-592e1da57bde",
      "metadata": {
        "id": "4174f629-9688-4227-bc67-592e1da57bde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4d8a911-20bf-4705-f6c4-bee5502832e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7f58eba3529f>:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  fbs_data_filter_one['element'] = fbs_data_filter_one['element'].replace(\n"
          ]
        }
      ],
      "source": [
        "#renamed columns\n",
        "fbs_data = fbs_raw_data.rename(columns={\n",
        "    'Area Code': 'area_code',\n",
        "    'Area Code (M49)': 'area_code_m49',\n",
        "    'Area': 'area',\n",
        "    'Item Code': 'item_code',\n",
        "    'Item Code (FBS)': 'item_code_fbs',\n",
        "    'Item': 'item',\n",
        "    'Element Code': 'element_code',\n",
        "    'Element': 'element',\n",
        "    'Year Code': 'year_code',\n",
        "    'Year': 'year',\n",
        "    'Unit': 'unit',\n",
        "    'Value': 'value',\n",
        "    'Flag': 'flag'})\n",
        "\n",
        "\n",
        "#filter conditions\n",
        "cond1 = fbs_data.item_code.isin([2511, 2514, 2555, 2805]) #getting only crops itens (Wheat, maize soybeans, rice)\n",
        "cond2 = fbs_data.area_code < 5000 #getting completely food balance sheets\n",
        "cond3 = fbs_data.year_code >= 2000\n",
        "#cond4 = fbs_data.year_code <= 2010\n",
        "cond5 = fbs_data.element_code.isin([645, 664, 674, 684])\n",
        "condf = cond1 & cond2 & cond3 & ~cond5 #& cond4\n",
        "\n",
        "fbs_data_filter_one = fbs_data.loc[condf]\n",
        "\n",
        "#renaming rows in element column\n",
        "fbs_data_filter_one['element'] = fbs_data_filter_one['element'].replace(\n",
        "    {'Production': 'fbs_production',\n",
        "     'Import Quantity': 'fbs_import_quantity',\n",
        "     'Stock Variation': 'fbs_stock_variation',\n",
        "     'Domestic supply quantity': 'fbs_domestic_supply_quantity',\n",
        "     'Seed': 'fbs_seed',\n",
        "     'Losses': 'fbs_losses',\n",
        "     'Food': 'fbs_food',\n",
        "     'Export Quantity': 'fbs_export_quantity',\n",
        "     'Feed': 'fbs_feed',\n",
        "     'Other uses (non-food)': 'fbs_other_uses',\n",
        "     'Processing': 'fbs_processing'})\n",
        "\n",
        "#adding populations\n",
        "cond1 = fbs_data.item_code.isin([2501])\n",
        "condf = cond1\n",
        "\n",
        "fbs_data_filter_two = fbs_data.loc[condf].rename(columns={'value': 'fbs_population'})\n",
        "fbs_data_filter_two = fbs_data_filter_two[['area_code', 'year_code', 'fbs_population']]\n",
        "\n",
        "#pivoting table\n",
        "fbs_pivot = fbs_data_filter_one.pivot_table(values = 'value', index = ['area_code', 'year_code'], columns = ['element'], aggfunc=np.sum)\n",
        "fbs_pivot = pd.DataFrame(fbs_pivot.to_records())\n",
        "fbs_pivot_merged = pd.merge(fbs_pivot, fbs_data_filter_two, on=['area_code', 'year_code'], how=\"inner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Food Security data transforming\n"
      ],
      "metadata": {
        "id": "rlzLmqjppULY"
      },
      "id": "rlzLmqjppULY"
    },
    {
      "cell_type": "code",
      "source": [
        "fsi_data = fsi_raw_data.rename(columns={\n",
        "    'Area Code': 'area_code',\n",
        "    'Area': 'area',\n",
        "    'Area Code (M49)': 'area_code_m49',\n",
        "    'Item Code': 'item_code',\n",
        "    'Item': 'item',\n",
        "    'Element Code': 'element_code',\n",
        "    'Element': 'element',\n",
        "    'Year Code': 'year_code',\n",
        "    'Year': 'year',\n",
        "    'Unit': 'unit',\n",
        "    'Value': 'value',\n",
        "    'Flag': 'flag',\n",
        "    'Note': 'note'})\n",
        "\n",
        "#filter conditions\n",
        "cond1 = fsi_data.element_code.isin([6121]) #getting main values and excluding confidence interval\n",
        "condf = cond1\n",
        "\n",
        "fbs_data_filter_one = fsi_data.loc[condf]\n",
        "\n",
        "#renaming variables\n",
        "path1 = '/content/drive/MyDrive/ia_ml_projects/post_harvest_loss/auxiliary_files/fbi_item_features_renamed.xlsx'\n",
        "fsi_features_renamed = pd.read_excel(path1)\n",
        "fsi_features_renamed = fsi_features_renamed[['item_code', 'item_renamed']]\n",
        "\n",
        "#merging dataset with new features renamed\n",
        "fbs_data_filter_one['item_code'] = fbs_data_filter_one['item_code'].astype(str)\n",
        "fsi_features_renamed['item_code'] = fsi_features_renamed['item_code'].astype(str)\n",
        "\n",
        "fsi_merged = pd.merge(fbs_data_filter_one, fsi_features_renamed, on=['item_code'], how=\"left\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncyJTDHcI4Mf",
        "outputId": "57046cdf-cc03-4311-be90-0ff702fda0b2"
      },
      "id": "ncyJTDHcI4Mf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-81284c3bdeae>:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  fbs_data_filter_one['item_code'] = fbs_data_filter_one['item_code'].astype(str)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_last_or_first(x):\n",
        "  \"\"\"\n",
        "  Returns the last element of a tuple if it has more than one element,\n",
        "  otherwise returns the first element.\n",
        "\n",
        "  Args:\n",
        "      x: A tuple.\n",
        "\n",
        "  Returns:\n",
        "      The last element of the tuple if it has more than one element,\n",
        "      otherwise the first element.\n",
        "  \"\"\"\n",
        "  if len(x) > 1:\n",
        "    return x[-1]\n",
        "  else:\n",
        "    return x[0]\n"
      ],
      "metadata": {
        "id": "yBhUXQAxnNq2"
      },
      "id": "yBhUXQAxnNq2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correcting the moving average of the years\n",
        "fsi_merged['year'] = fsi_merged['year'].str.split('-')\n",
        "fsi_merged['year_code'] = fsi_merged.year.apply(get_last_or_first)\n",
        "\n",
        "#pivoting table\n",
        "fsi_pivot = fsi_merged.pivot(values = 'value', index = ['area_code', 'year_code'], columns = ['item_renamed'])\n",
        "fsi_pivot = fsi_pivot.reset_index()\n",
        "fsi_pivot['year_code'] = fsi_pivot['year_code'].astype('int64')"
      ],
      "metadata": {
        "id": "ah0wmvyknPAT"
      },
      "id": "ah0wmvyknPAT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_less_than_symbol(df):\n",
        "  \"\"\"\n",
        "  This function iterates through each column in a dataframe and removes the '<' symbol from any string values.\n",
        "\n",
        "  Args:\n",
        "      df: The pandas dataframe to process.\n",
        "\n",
        "  Returns:\n",
        "      A new pandas dataframe with the '<' symbol removed from all string columns.\n",
        "  \"\"\"\n",
        "\n",
        "  for col in df.columns:\n",
        "    # Check if the column contains string values\n",
        "    if df[col].dtype == \"object\":\n",
        "      # Remove the '<' symbol from string values\n",
        "      df[col] = df[col].str.replace(\"<\", \"\")\n",
        "\n",
        "  return df\n",
        "\n",
        "# Apply the function to the data_merged dataframe\n",
        "fsi_pivot = remove_less_than_symbol(fsi_pivot)\n",
        "fsi_pivot = fsi_pivot.apply(pd.to_numeric)"
      ],
      "metadata": {
        "id": "H_PwnIkugmd-"
      },
      "id": "H_PwnIkugmd-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f5ca55c4-5cec-4d0b-bf3a-625fbcb95b75",
      "metadata": {
        "id": "f5ca55c4-5cec-4d0b-bf3a-625fbcb95b75"
      },
      "source": [
        "### Trade indices data transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16428e15-4827-4dfd-b681-0642d634e80e",
      "metadata": {
        "id": "16428e15-4827-4dfd-b681-0642d634e80e"
      },
      "outputs": [],
      "source": [
        "trade_data = trade_raw_data.rename(columns={\n",
        "    'Area Code': 'area_code',\n",
        "    'Area': 'area',\n",
        "    'Item Code': 'item_code',\n",
        "    'Item': 'item',\n",
        "    'Element Code': 'element_code',\n",
        "    'Element': 'element',\n",
        "    'Year Code': 'year_code',\n",
        "    'Year': 'year',\n",
        "    'Unit': 'unit',\n",
        "    'Value': 'value',\n",
        "    'Flag': 'flag'})\n",
        "\n",
        "#filter conditions\n",
        "cond1 = trade_data.element_code.isin([64, 65, 94, 95])\n",
        "#cond2 = trade_data.year_code >= 2000\n",
        "#cond3 = trade_data.year_code <= 2010\n",
        "condf = cond1 #& cond2 & cond3\n",
        "\n",
        "trade_data = trade_data.loc[condf]\n",
        "\n",
        "##renaming rows in element column\n",
        "trade_data['element'] = trade_data['element'].replace(\n",
        "    {'Import Value Base Period Quantity': 'trade_import_value_qtty',\n",
        "     'Import Value Base Period Price': 'trade_import_value_price',\n",
        "     'Export Value Base Quantity': 'trade_export_value_qtt',\n",
        "     'Export Value Base Price': 'trade_export_value_price'})\n",
        "\n",
        "##pivoting table\n",
        "trade_pivot = trade_data.pivot_table(values = 'value', index = ['area_code', 'year_code'], columns = ['element'], aggfunc=np.sum)\n",
        "trade_pivot = pd.DataFrame(trade_pivot.to_records())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae27d3a-d213-4ce0-a35a-45d720cb5183",
      "metadata": {
        "id": "9ae27d3a-d213-4ce0-a35a-45d720cb5183"
      },
      "source": [
        "### Merging data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#merging tables\n",
        "data_merged = pd.merge(fbs_pivot,countries_list.rename(columns={'code_faostat': 'area_code'}), on=['area_code'])\n",
        "data_merged = pd.merge(data_merged,fsi_pivot, on=['area_code', 'year_code'])\n",
        "data_merged = pd.merge(data_merged,trade_pivot, on=['area_code', 'year_code'])\n",
        "\n",
        "#filtering the same countries as the study: GUSTAVSSON, J. et al. Global food losses and food waste – Extent, causes and prevention. Rome, FAO, 2011.\n",
        "cond1 = data_merged.gustavsson == 1\n",
        "condf = cond1\n",
        "\n",
        "data_merged = data_merged.loc[condf]"
      ],
      "metadata": {
        "id": "F2tSnQNf-R-h"
      },
      "id": "F2tSnQNf-R-h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balancing and filling gaps"
      ],
      "metadata": {
        "id": "sZGiZHmdtWA_"
      },
      "id": "sZGiZHmdtWA_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing columns with many missing values"
      ],
      "metadata": {
        "id": "dngQxj9xkzPM"
      },
      "id": "dngQxj9xkzPM"
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop columns with all null values\n",
        "#data_merged.dropna(axis=1, how=\"all\", inplace=True)\n",
        "\n",
        "# Calculate the percentage of missing values in each column\n",
        "missing_values = data_merged.isna().sum() / len(data_merged)\n",
        "\n",
        "# Select columns with more than 50% missing values\n",
        "columns_to_drop = missing_values[missing_values > 0.35].index.tolist()\n",
        "print(columns_to_drop)\n",
        "\n",
        "# Drop the selected columns\n",
        "data_merged = data_merged.drop(columns=columns_to_drop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzuaO3cKVHjW",
        "outputId": "2a8e25a0-0d2b-48e2-fa15-76948fa529db"
      },
      "id": "ZzuaO3cKVHjW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fbi_children_affected_wasting_perc', 'fbi_infants_breastfeeding_perc', 'fbi_people_undernourished_perc', 'fbi_severe_food_insecure_famele_people_perc', 'fbi_severe_food_insecure_famele_people_perc_mm', 'fbi_severe_food_insecure_male_people_perc', 'fbi_severe_food_insecure_male_people_perc_mm', 'fbi_severe_food_insecure_rural_people_perc', 'fbi_severe_food_insecure_semi_dense_people_perc', 'fbi_severe_food_insecure_urban_people_perc', 'fbi_severe_moderate_food_insecure_famele_people_perc', 'fbi_severe_moderate_food_insecure_famele_people_perc_mm', 'fbi_severe_moderate_food_insecure_male_people_perc', 'fbi_severe_moderate_food_insecure_male_people_perc_mm', 'fbi_severe_moderate_food_insecure_people_perc', 'fbi_severe_moderate_food_insecure_people_perc_mm', 'fbi_severe_moderate_food_insecure_rural_people_perc', 'fbi_severe_moderate_food_insecure_semi_dense_people_perc', 'fbi_severe_moderate_food_insecure_urban_people_perc', 'fbi_share_severe_food_insecure_pop_perc', 'fbi_share_severe_food_insecure_pop_perc_mm']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a summary table for the region to fill in the largest gaps"
      ],
      "metadata": {
        "id": "yqqtcELsk40L"
      },
      "id": "yqqtcELsk40L"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the columns to drop\n",
        "columns_to_drop = [\"country\", \"code_c\", \"region\", \"income_group\", \"code_g\"]\n",
        "\n",
        "# Drop the specified columns from the dataframe\n",
        "data_merged_drop = data_merged.drop(columns=columns_to_drop)\n",
        "data_region_median = data_merged_drop.groupby(['code_r', 'year_code']).median().reset_index()"
      ],
      "metadata": {
        "id": "8AYSgzcnZRzB"
      },
      "id": "8AYSgzcnZRzB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_gaps_with_group_mean(df, variable_list, groupby_column):\n",
        "  \"\"\"\n",
        "  Fills missing values in specified columns of a dataframe with the mean value within each group.\n",
        "\n",
        "  Args:\n",
        "      df: The pandas dataframe to process.\n",
        "      variable_list: A list of column names where missing values should be filled.\n",
        "      groupby_column: The name of the column to group by.\n",
        "\n",
        "  Returns:\n",
        "      A new pandas dataframe with missing values filled with the mean value within each group.\n",
        "  \"\"\"\n",
        "\n",
        "  # Iterate through each variable in the list\n",
        "  for variable in variable_list:\n",
        "    # Fill missing values in the variable using the groupby mean\n",
        "    df[variable] = df[variable].fillna(df.groupby(groupby_column)[variable].transform('mean'))\n",
        "\n",
        "  # Return the updated dataframe\n",
        "  return df\n",
        "\n",
        "# Define the list of variables with gaps and the groupby column\n",
        "nan_cols = [i for i in data_region_median.columns if data_region_median[i].isnull().any()]\n",
        "\n",
        "variable_list = nan_cols\n",
        "groupby_column = \"code_r\"\n",
        "\n",
        "# Apply the function to the data_merged dataframe\n",
        "data_merged_filled = fill_gaps_with_group_mean(data_region_median, variable_list, groupby_column)"
      ],
      "metadata": {
        "id": "05JUim6fpGRr"
      },
      "id": "05JUim6fpGRr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filling in missing values in the main table"
      ],
      "metadata": {
        "id": "Ynorebb1tCzV"
      },
      "id": "Ynorebb1tCzV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validating FBS data"
      ],
      "metadata": {
        "id": "YIisNtUT-PHa"
      },
      "id": "YIisNtUT-PHa"
    },
    {
      "cell_type": "code",
      "source": [
        "#This idea was abandoned because the data panel is unbalanced due to the impossibility of being (countries that were born, countries that started to have statistics) and not due to a lack of data!\n",
        "\n",
        "\n",
        "#Identify all countries and years present\n",
        "#countries = data_merged['area_code'].unique()\n",
        "#years = range(data_merged['year_code'].min(), data_merged['year_code'].max() + 1)\n",
        "\n",
        "#Create a full dataFrame with all combinations of country and year\n",
        "#data_balanced = pd.MultiIndex.from_product([countries, years], names=['area_code', 'year_code']).to_frame(index=False)\n",
        "\n",
        "#data_balanced = pd.merge(data_balanced,data_merged, on=['area_code', 'year_code'], how='left')"
      ],
      "metadata": {
        "id": "VuKMnUC-xyTs"
      },
      "id": "VuKMnUC-xyTs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_missing_with_zero(df, column_substring):\n",
        "  \"\"\"\n",
        "  Fills missing values in all columns of a dataframe whose names contain a specific substring with 0.\n",
        "\n",
        "  Args:\n",
        "      df: The pandas dataframe to process.\n",
        "      column_substring: The substring to search for in column names.\n",
        "\n",
        "  Returns:\n",
        "      A new pandas dataframe with missing values filled with 0 in the specified columns.\n",
        "  \"\"\"\n",
        "\n",
        "  # Identify columns that contain the substring\n",
        "  columns_to_fill = [col for col in df.columns if column_substring in col]\n",
        "\n",
        "  # Fill missing values in the selected columns with 0\n",
        "  for col in columns_to_fill:\n",
        "    df[col] = df[col].fillna(0)\n",
        "\n",
        "  # Return the updated dataframe\n",
        "  return df\n",
        "\n",
        "# Define the substring to search for\n",
        "column_substring = \"fbs\"\n",
        "\n",
        "# Fill missing values with 0 in the specified columns\n",
        "data_merged_filled = fill_missing_with_zero(data_merged, column_substring)"
      ],
      "metadata": {
        "id": "cMDA1Mxt-FXD"
      },
      "id": "cMDA1Mxt-FXD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FBS basic identities according to FAO**\n",
        "\n",
        "As many countries do not collect data on stock levels for the majority of products, absolute opening and closing stock levels are replaced by estimate of the change in stock levels during the reference period.\n",
        "\n",
        "**a) Domestic supply = Domestic utilization**\n",
        "\n",
        "Production + Imports – Exports – ΔStocks = Food + Feed + Seed + Tourist Food + Industrial Use + Loss + Residual Use\n",
        "\n",
        "**b) Total supply = Total utilization**\n",
        "\n",
        "Production + Imports – ΔStocks = Exports + Food + Feed + Seed + Tourist Food + Industrial Use + Loss + Residual Use\n"
      ],
      "metadata": {
        "id": "frBDSKem_o9-"
      },
      "id": "frBDSKem_o9-"
    },
    {
      "cell_type": "code",
      "source": [
        "data_merged_filled['domestic_supply'] = (  data_merged_filled['fbs_production']\n",
        "                                         + data_merged_filled['fbs_import_quantity']\n",
        "                                         - data_merged_filled['fbs_export_quantity']\n",
        "                                         + data_merged_filled['fbs_stock_variation'])\n",
        "\n",
        "data_merged_filled['domestic_utilization'] = (  data_merged_filled['fbs_food']\n",
        "                                              + data_merged_filled['fbs_feed']\n",
        "                                              + data_merged_filled['fbs_seed']\n",
        "                                              + data_merged_filled['fbs_processing']\n",
        "                                              + data_merged_filled['fbs_other_uses']\n",
        "                                              + data_merged_filled['fbs_losses'])\n",
        "\n",
        "data_merged_filled['diff_identities'] = data_merged_filled['domestic_supply'] - data_merged_filled['domestic_utilization']"
      ],
      "metadata": {
        "id": "_HICly3cxzfx"
      },
      "id": "_HICly3cxzfx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filling gaps in FBI data"
      ],
      "metadata": {
        "id": "HhjEnmOjYIYv"
      },
      "id": "HhjEnmOjYIYv"
    },
    {
      "cell_type": "code",
      "source": [
        "def fill_fbi_first_two_rows_with_area_mean(df):\n",
        "  \"\"\"\n",
        "  Fills missing values in the first two rows of FBI columns for each area code with the mean of the area code.\n",
        "\n",
        "  Args:\n",
        "      df: The pandas DataFrame.\n",
        "\n",
        "  Returns:\n",
        "      A new DataFrame with missing values filled.\n",
        "  \"\"\"\n",
        "\n",
        "  # Identify FBI columns.\n",
        "  fbi_columns = [col for col in df.columns if col.startswith('fbi')]\n",
        "\n",
        "  # Create a new DataFrame to store the transformed data.\n",
        "  df_transformed = df.copy()\n",
        "\n",
        "  # Group by area code.\n",
        "  grouped_df = df.groupby('area_code')\n",
        "\n",
        "  # Iterate over area codes.\n",
        "  for area_code, group in grouped_df:\n",
        "    # Get the first two rows of the current area code.\n",
        "    first_two_rows = group.head(2)\n",
        "\n",
        "    # Check if any of the FBI columns have NaN values in the first two rows.\n",
        "    if first_two_rows[fbi_columns].isna().any().any():\n",
        "      # Calculate the area code mean for each FBI column.\n",
        "      area_means = group[fbi_columns].mean().to_dict()\n",
        "\n",
        "      # Fill missing values in the first two rows for each FBI column.\n",
        "      for col in fbi_columns:\n",
        "        first_two_rows.loc[first_two_rows[col].isna(), col] = area_means[col]\n",
        "\n",
        "      # Update the transformed DataFrame with the filled first two rows.\n",
        "      df_transformed.loc[first_two_rows.index] = first_two_rows\n",
        "\n",
        "  return df_transformed\n",
        "\n",
        "data_merged_filled = fill_fbi_first_two_rows_with_area_mean(data_merged_filled)"
      ],
      "metadata": {
        "id": "yDEF_iy6Kan-"
      },
      "id": "yDEF_iy6Kan-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filling gaps by region median"
      ],
      "metadata": {
        "id": "YM174hteYRhs"
      },
      "id": "YM174hteYRhs"
    },
    {
      "cell_type": "code",
      "source": [
        "# Identificar colunas comuns\n",
        "common_columns = list(set(data_merged_filled.columns) & set(data_region_median.columns))\n",
        "\n",
        "# Excluir colunas de agrupamento das colunas comuns, se estiverem presentes\n",
        "grouping_columns = ['code_r', 'year_code']\n",
        "common_columns = [col for col in common_columns if col not in grouping_columns]\n",
        "\n",
        "# Agrupar o dataset data_region_median por 'code_r' e 'year_code' e calcular a mediana\n",
        "region_median_grouped = data_region_median.groupby(['code_r', 'year_code'])[common_columns].median().reset_index()\n",
        "\n",
        "# Preencher os valores faltantes no dataset data_merged_filled\n",
        "for col in common_columns:\n",
        "    data_merged_filled[col] = data_merged_filled.apply(\n",
        "        lambda row: region_median_grouped[\n",
        "            (region_median_grouped['code_r'] == row['code_r']) &\n",
        "            (region_median_grouped['year_code'] == row['year_code'])\n",
        "        ][col].values[0] if pd.isna(row[col]) else row[col], axis=1\n",
        "    )"
      ],
      "metadata": {
        "id": "1MoYEIxfU0xb"
      },
      "id": "1MoYEIxfU0xb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_region_median.to_excel('data_region_median.xlsx')\n",
        "from google.colab import files\n",
        "files.download('data_region_median.xlsx')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "x1ChJGNwyuvC",
        "outputId": "628caa01-5fd4-4d7d-ce17-47eca5ce45ba"
      },
      "id": "x1ChJGNwyuvC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e77264e5-e187-414a-9ca4-398b722af911\", \"data_region_median.xlsx\", 26699)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading"
      ],
      "metadata": {
        "id": "2A4TXRo3Yq8j"
      },
      "id": "2A4TXRo3Yq8j"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precisa fazer as variáveis dummies de região e phl, abertura comércio exterior, etc..\n"
      ],
      "metadata": {
        "id": "Q2i7KbrVYxmc"
      },
      "id": "Q2i7KbrVYxmc"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}