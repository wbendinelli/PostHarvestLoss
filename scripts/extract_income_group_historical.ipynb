{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75f19b80-cfb2-4271-8b4f-98c80b2d7810",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the path to the Excel file\n",
    "# https://datahelpdesk.worldbank.org/knowledgebase/articles/906519-world-bank-country-and-lending-groups\n",
    "file = '/Workspace/Users/wbendinelli@gmail.com/PostHarvestLoss/data/external/income_group_historical.xlsx'\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Read the Excel file from the specified path using the openpyxl engine.\n",
    "# - sheet_name: \"Country Analytical History\" (the sheet to read)\n",
    "# - skiprows: Skip rows 0 and 4 (rows that should not be read)\n",
    "# - header: Use row 3 as the header row\n",
    "# - dtype: Read all columns as string type\n",
    "df = pd.read_excel(\n",
    "    file,\n",
    "    sheet_name='Country Analytical History',\n",
    "    skiprows=[0, 4],\n",
    "    header=3,\n",
    "    engine='openpyxl',\n",
    "    dtype=str\n",
    ")\n",
    "\n",
    "# Remove the first 5 rows from the DataFrame and reset the index.\n",
    "df = df.iloc[5:].reset_index(drop=True)\n",
    "\n",
    "# Rename the first two columns:\n",
    "# - Rename the first column to \"code_c\"\n",
    "# - Rename the second column to \"country\"\n",
    "df = df.rename(columns={df.columns[0]: \"code_c\", df.columns[1]: \"country\"})\n",
    "\n",
    "# Drop rows where the \"code_c\" column has missing values.\n",
    "df = df.dropna(subset=[\"code_c\"])\n",
    "\n",
    "# Define the identifier columns that will remain unchanged during the melting process.\n",
    "id_vars = [\"code_c\", \"country\"]\n",
    "\n",
    "# All remaining columns (starting from the third column) will be melted into a new column.\n",
    "value_vars = df.columns[2:]\n",
    "\n",
    "# Transform the DataFrame from wide to long format using pd.melt.\n",
    "# - id_vars: Columns to keep as identifiers (\"code_c\" and \"country\")\n",
    "# - value_vars: Columns that will be unpivoted (years)\n",
    "# - var_name: Name for the new column that will hold the original column names (\"year\")\n",
    "# - value_name: Name for the new column that will hold the values (\"income_group\")\n",
    "df_long = pd.melt(df,\n",
    "                  id_vars=id_vars,\n",
    "                  value_vars=value_vars,\n",
    "                  var_name=\"year\",\n",
    "                  value_name=\"income_group\")\n",
    "\n",
    "# Drop rows where the \"income_group\" column is null.\n",
    "df_long = df_long.dropna(subset=[\"income_group\"])\n",
    "\n",
    "# Drop rows where the \"income_group\" column has the value \"..\"\n",
    "df_long = df_long[df_long[\"income_group\"] != \"..\"]\n",
    "\n",
    "# Define a mapping dictionary to rename the income group codes to full descriptions.\n",
    "mapping = {\n",
    "    'L': 'Low income',\n",
    "    'UM': 'Upper middle income',\n",
    "    'H': 'High income',\n",
    "    'LM': 'Lower middle income',\n",
    "    'LM*': 'Lower middle income'\n",
    "}\n",
    "\n",
    "# Replace the values in the \"income_group\" column using the mapping dictionary.\n",
    "df_long['income_group'] = df_long['income_group'].replace(mapping)\n",
    "\n",
    "# Define the mapping dictionary for code_g\n",
    "mapping_code_g = {\n",
    "    'Low income': 'LIC',\n",
    "    'Upper middle income': 'UMC',\n",
    "    'Lower middle income': 'LMC',\n",
    "    'High income': 'HIC'\n",
    "}\n",
    "# Create new column code_g by mapping the 'income_group' column using mapping_code_g\n",
    "df_long['code_g'] = df_long['income_group'].replace(mapping_code_g)\n",
    "\n",
    "# Convert the Pandas DataFrame (df_long) to a Spark DataFrame\n",
    "spark_df = spark.createDataFrame(df_long)\n",
    "\n",
    "# Write the Spark DataFrame as a Delta table in the Databricks database.\n",
    "# Adjust the table name as needed.\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(\"workspace.postharvestloss.aux_table_income_group_historical\")\n",
    "\n",
    "# Display the final transformed DataFrame.\n",
    "df_long.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "extract_income_group_historical",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
